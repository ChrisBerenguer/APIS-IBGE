{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv  # Para carregar variáveis de ambiente de um arquivo .env\n",
    "import os  # Para acessar variáveis de ambiente do sistema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if openai_api_key is None:\n",
    "    raise ValueError(\"A chave da API não foi encontrada no arquivo .env.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conceito geral do Streaming (chunks):**\n",
    "\n",
    "Chunks são pequenos pedaços (fragmentos) de uma resposta maior.\n",
    "\n",
    "Quando você usa a API da OpenAI com o parâmetro stream=True, o modelo não envia a resposta inteira de uma vez, ele vai enviando a resposta em partes, chamadas de chunks.\n",
    "\n",
    "Vantagens:\n",
    "\n",
    "- Respostas chegam mais rápido (token por token).\n",
    "- Você pode começar a exibir o texto pro usuário enquanto o modelo ainda está gerando o resto.\n",
    "- Melhora a experiência de interfaces ao estilo chat ao vivo, com \"digitação gradual\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passando argumento Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "mensagens = [{'role': 'user', 'content': 'explique o Reumo de Star Wars' }]\n",
    "resposta = client.chat.completions.create(\n",
    "    messages=mensagens,\n",
    "    model='gpt-3.5-turbo-0125',\n",
    "    max_tokens=1000,\n",
    "    temperature=0,\n",
    "    stream=True # Paremetro Stream\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3037161823.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[55], line 5\u001b[1;36m\u001b[0m\n\u001b[1;33m    stream_resposta.choices and hasattr(stream_resposta.choices[0].delta, 'content'):\u001b[0m\n\u001b[1;37m                                                                                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "resposta_completa = \"\"\n",
    "\n",
    "for stream_resposta in resposta:\n",
    "    try:\n",
    "        if stream_resposta.choices and hasattr(stream_resposta.choices[0].delta, 'content'):\n",
    "            texto = stream_resposta.choices[0].delta.content\n",
    "            resposta_completa += texto\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar chunk: {e}\")\n",
    "\n",
    "print(\"\\nResposta completa:\")\n",
    "print(resposta_completa)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
